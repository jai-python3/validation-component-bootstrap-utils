{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Validation Component Bootstrap Utils","text":"<p>Validation Component Bootstrap Utils is a comprehensive set of Python scripts and utilities designed to streamline the process of parsing and validating tab-delimited or comma-separated files. This toolkit automates the generation of essential modules, making it easier to integrate file validation components into your projects.</p> <p>Modules Generated</p> <p>validator.py: This module provides a robust set of functions and classes for validating data within the specified file format. It ensures that the content adheres to defined standards, offering a solid foundation for data integrity.</p> <p>parser.py: The parser module facilitates efficient extraction of data from tab-delimited or comma-separated files. It is engineered to handle various data structures and file layouts, ensuring flexibility and adaptability to diverse use cases.</p> <p>record.py: The record module defines structures for organizing and representing individual records within the files. It lays the groundwork for maintaining data consistency and ease of manipulation during the parsing and validation processes.</p> <p>file_validation.py: This module orchestrates the validation and parsing processes, providing a unified interface for users. It acts as the entry point for utilizing the toolkit, promoting simplicity and coherence in handling file operations.</p>"},{"location":"#commands","title":"Commands","text":"<p>The following exported scripts are available:</p> <ul> <li>generate-validation-module</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>GitHub</li> <li>PYPI</li> </ul>"},{"location":"CHANGELOG/","title":"CHANGELOG","text":""},{"location":"CHANGELOG/#v030-2024-02-05","title":"v0.3.0 - 2024-02-05","text":"<p>ADDS: - support for processing comma-separated input file</p> <p>FIXES: - support for normalizing the Enum names - support for deriving the module namespace for the file_validation.py executable</p>"},{"location":"CHANGELOG/#v021-2023-12-19","title":"v0.2.1 - 2023-12-19","text":"<p>ADDS: - missing dependencies</p>"},{"location":"CHANGELOG/#v020-2023-12-19","title":"v0.2.0 - 2023-12-19","text":"<p>ADDS: - support for generating primary executable from template - use cases diagram</p>"},{"location":"INSTALL/","title":"INSTALL","text":""},{"location":"INSTALL/#install-from-pypi","title":"Install from PYPI","text":"<p>Now you can install your package in your Python virtual environment</p> <pre><code>pip install validation-component-bootstrap-utils\n</code></pre>"},{"location":"INSTALL/#clone-project","title":"Clone project","text":"<p>You can <code>git clone</code> this project.</p> <pre><code>git clone https://github.com/jai-python3/validation-component-bootstrap-utils.git\ncd validation-component-bootstrap-utils\n</code></pre>"},{"location":"INSTALL/#local-pip-install","title":"Local pip install","text":"<p>You can optionally establish a Python virtual environment. Then you can run the <code>setup.py</code> script to build to project and then run <code>pip install</code> to install in your local Python virtual environment.</p> <pre><code>virtualenv -p python3 venv\nsource venv/bin/activate\npython setup.py sdist\npip install .\n</code></pre>"},{"location":"INSTALL/#uninstall","title":"Uninstall","text":"<p>You can uninstall like this:</p> <pre><code>source venv/bin/activate\npip uninstall validation-component-bootstrap-utils\nmake clean\n</code></pre>"},{"location":"INSTALL/#developers","title":"Developers","text":"<p>If you modify the code in this package in your local virtual environment:</p> <pre><code>pip uninstall validation-component-bootstrap-utils\nmake clean\npython setup.py sdist\npip install .\n</code></pre>"},{"location":"INSTALL/#publish-to-pypi","title":"Publish to PYPI","text":"<p>You want can publish the code in this package to the PYPI repository.</p>"},{"location":"INSTALL/#install-twine-and-setuptools","title":"Install twine and setuptools","text":"<p>Install <code>twine</code> and <code>setuptools</code>.</p> <pre><code>pip install twine setuptools\n</code></pre>"},{"location":"INSTALL/#build-the-distribution-package","title":"Build the Distribution Package","text":"<pre><code>python setup.py sdist bdist_wheel\n</code></pre>"},{"location":"INSTALL/#configure-your-pypirc","title":"Configure your ~/.pypirc:","text":"<pre><code>[pypi]\n  username = __token__\n  password = pypi-YOUR-TOKEN\n</code></pre>"},{"location":"INSTALL/#upload-your-package-to-pypi","title":"Upload Your Package to PyPI","text":"<pre><code>twine upload dist/*\n</code></pre>"},{"location":"TODO/","title":"TODO","text":"<ul> <li>Create class diagram</li> <li>Add support for deriving the datatype (integer, float, string, boolean)</li> <li>Implement unit tests (pytest)</li> </ul>"},{"location":"generate_validation_module/","title":"Generate Validation Module module","text":"<p>Parse the file and generate the validation modules.</p>"},{"location":"generate_validation_module/#validation_component_bootstrap_utils.generate_validation_module.check_infile_status","title":"<code>check_infile_status(infile=None, extension=None)</code>","text":"<p>Check if the file exists, if it is a regular file and whether it has content.</p> <p>Parameters:</p> Name Type Description Default <code>infile</code> <code>str</code> <p>the file to be checked</p> <code>None</code> Source code in <code>validation_component_bootstrap_utils/generate_validation_module.py</code> <pre><code>def check_infile_status(\n    infile: str | None = None, extension: str | None = None\n) -&gt; None:\n    \"\"\"Check if the file exists, if it is a regular file and whether it has\n    content.\n\n    Args:\n        infile (str): the file to be checked\n\n    Raises:\n        None\n    \"\"\"\n\n    error_ctr = 0\n\n    if infile is None or infile == \"\":\n        error_console.print(f\"'{infile}' is not defined\")\n        error_ctr += 1\n    else:\n        if not os.path.exists(infile):\n            error_ctr += 1\n            error_console.print(f\"'{infile}' does not exist\")\n        else:\n            if not os.path.isfile(infile):\n                error_ctr += 1\n                error_console.print(f\"'{infile}' is not a regular file\")\n            if os.stat(infile).st_size == 0:\n                error_console.print(f\"'{infile}' has no content\")\n                error_ctr += 1\n            if extension is not None and not infile.endswith(extension):\n                error_console.print(\n                    f\"'{infile}' does not have filename extension '{extension}'\"\n                )\n                error_ctr += 1\n\n    if error_ctr &gt; 0:\n        error_console.print(f\"Detected problems with input file '{infile}'\")\n        sys.exit(1)\n</code></pre>"},{"location":"generate_validation_module/#validation_component_bootstrap_utils.generate_validation_module.main","title":"<code>main(config_file, data_file_type, infile, logfile, namespace, outdir, outfile, template_path, verbose)</code>","text":"<p>Parse the file and generate the validation modules.</p> Source code in <code>validation_component_bootstrap_utils/generate_validation_module.py</code> <pre><code>@click.command()  # type: ignore\n@click.option(\n    \"--config_file\",\n    type=click.Path(exists=True),\n    help=f\"The configuration file for this project - default is '{DEFAULT_CONFIG_FILE}'\",\n)  # type: ignore\n@click.option(\"--data_file_type\", help=\"Required: The type of the data files to be processed\")  # type: ignore\n@click.option(\"--infile\", help=\"The primary input file\")  # type: ignore\n@click.option(\"--logfile\", help=\"The log file\")  # type: ignore\n@click.option(\"--namespace\", help=\"Required: The namespace will dictate relative placement of the modules\")  # type: ignore\n@click.option(\n    \"--outdir\",\n    help=\"The default is the current working directory - default is '{DEFAULT_OUTDIR}'\",\n)  # type: ignore\n@click.option(\"--outfile\", help=\"The output final report file\")  # type: ignore\n@click.option(\n    \"--template_path\",\n    help=f\"The directory containing the Jinja2 template files - default is '{DEFAULT_TEMPLATE_PATH}'\",\n)  # type: ignore\n@click.option(\n    \"--verbose\",\n    is_flag=True,\n    help=f\"Will print more info to STDOUT - default is '{DEFAULT_VERBOSE}'\",\n)  # type: ignore\ndef main(\n    config_file: str,\n    data_file_type: str,\n    infile: str,\n    logfile: str,\n    namespace: str,\n    outdir: str,\n    outfile: str,\n    template_path: str,\n    verbose: bool,\n) -&gt; None:\n    \"\"\"Parse the file and generate the validation modules.\"\"\"\n    error_ctr = 0\n\n    if infile is None:\n        error_console.print(\"--infile was not specified\")\n        error_ctr += 1\n\n    if data_file_type is None:\n        error_console.print(\"--data_file_type was not specified\")\n        error_ctr += 1\n\n    if namespace is None:\n        error_console.print(\"--namespace was not specified\")\n        error_ctr += 1\n\n    if error_ctr &gt; 0:\n        sys.exit(1)\n\n    check_infile_status(infile)\n\n    if config_file is None:\n        config_file = DEFAULT_CONFIG_FILE\n        console.print(\n            f\"[yellow]--config_file was not specified and therefore was set to '{config_file}'[/]\"\n        )\n\n    check_infile_status(config_file, extension=\"yaml\")\n\n    if outdir is None:\n        outdir = DEFAULT_OUTDIR\n        console.print(\n            f\"[yellow]--outdir was not specified and therefore was set to '{outdir}'[/]\"\n        )\n\n    if not os.path.exists(outdir):\n        pathlib.Path(outdir).mkdir(parents=True, exist_ok=True)\n        console.print(f\"[yellow]Created output directory '{outdir}'[/]\")\n\n    if template_path is None:\n        template_path = DEFAULT_TEMPLATE_PATH\n        console.print(\n            f\"[yellow]--template_path was not specified and therefore was set to '{template_path}'[/]\"\n        )\n\n    if not os.path.exists(template_path):\n        console.print(\n            f\"[bold red]template_path '{template_path}' does not exist[/]\"\n        )\n        sys.exit(1)\n\n    if not os.path.isdir(template_path):\n        console.print(\n            f\"[bold red]template_path '{template_path}' is not a regular directory[/]\"\n        )\n        sys.exit(1)\n\n    if verbose is None:\n        verbose = DEFAULT_VERBOSE\n        console.print(f\"[yellow]--verbose was not specified and therefore was set to '{verbose}'[/]\")\n\n    if logfile is None:\n        logfile = os.path.join(\n            outdir, os.path.splitext(os.path.basename(__file__))[0] + \".log\"\n        )\n        console.print(\n            f\"[yellow]--logfile was not specified and therefore was set to '{logfile}'[/]\"\n        )\n\n    logfile = os.path.abspath(logfile)\n\n    logging.basicConfig(\n        format=DEFAULT_LOGGING_FORMAT,\n        level=DEFAULT_LOGGING_LEVEL,\n        filename=logfile,\n    )\n\n    # Read the configuration from the YAML file and\n    # load into dictionary.\n    logging.info(f\"Loading configuration from '{config_file}'\")\n\n    logging.info(\"Will load contents of config file 'config_file'\")\n    config = yaml.safe_load(Path(config_file).read_text())\n\n    manager = Manager(\n        config=config,\n        config_file=config_file,\n        data_file_type=data_file_type,\n        logfile=logfile,\n        outdir=outdir,\n        outfile=outfile,\n        namespace=namespace,\n        template_path=template_path,\n        verbose=verbose,\n    )\n\n    manager.generate_validation_modules(infile=os.path.abspath(infile))\n\n    print(f\"The log file is '{logfile}'\")\n    console.print(\n        f\"[bold green]Execution of '{os.path.abspath(__file__)}' completed[/]\"\n    )\n</code></pre>"},{"location":"manager/","title":"Manager module","text":""},{"location":"manager/#validation_component_bootstrap_utils.manager.Manager","title":"<code>Manager</code>","text":"<p>Class for managing the creation of the validation modules.</p> Source code in <code>validation_component_bootstrap_utils/manager.py</code> <pre><code>class Manager:\n    \"\"\"Class for managing the creation of the validation modules.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Constructor for Manager.\"\"\"\n        self.config = kwargs.get(\"config\", None)\n        self.config_file = kwargs.get(\"config_file\", None)\n        self.data_file_type = kwargs.get(\"data_file_type\", None)\n        self.logfile = kwargs.get(\"logfile\", None)\n        self.namespace = kwargs.get(\"namespace\", None)\n        self.outdir = kwargs.get(\"outdir\", None)\n        self.template_path = kwargs.get(\"template_path\", None)\n        self.verbose = kwargs.get(\"verbose\", DEFAULT_VERBOSE)\n\n        # Define a regular expression pattern to match special characters\n        self.pattern = r\"[^a-zA-Z0-9\\s]\"  # This pattern will keep alphanumeric characters and whitespace\n\n        self.column_name_to_attribute_name_lookup = {}\n        self.max_equality_values = self.config[\"max_equality_values\"]\n\n        self._init_templating_system()\n\n        logging.info(f\"Instantiated Manager in file '{os.path.abspath(__file__)}'\")\n\n    def _init_templating_system(self) -&gt; None:\n        \"\"\"Initialize the Jinja2 templating loader and environment.\"\"\"\n        # Specify the path to the templates directory\n        template_path = self.template_path\n\n        if not os.path.exists(template_path):\n            logging.error(f\"template path '{template_path}' does not exist\")\n            sys.exit(1)\n\n        # Create a FileSystemLoader and pass the template path to it\n        loader = FileSystemLoader(template_path)\n\n        # Create a Jinja2 Environment using the loader\n        self.env = Environment(loader=loader)\n\n    def generate_validation_modules(self, infile: str) -&gt; None:\n        \"\"\"Generate the validation modules for the specified file.\n\n        Args:\n            infile (str): the input tab-delimited or csv file\n        Returns:\n            None\n        \"\"\"\n        logging.info(\n            f\"Will attempt to generate validation modules for input file '{infile}'\"\n        )\n        if not os.path.exists(infile):\n            raise Exception(f\"file '{infile}' does not exist\")\n\n        extension = os.path.splitext(infile)[1]\n\n        if extension == \".csv\":\n            self._generate_validation_modules_for_tsv_file(infile, is_tsv=False)\n        elif extension == \".tsv\":\n            self._generate_validation_modules_for_tsv_file(infile, is_tsv=True)\n        else:\n            logging.error(\n                f\"Support does not exist for files with extension '{extension}'\"\n            )\n            sys.exit(1)\n\n    def _generate_validation_modules_for_tsv_file(self, infile: str, is_tsv: bool = True) -&gt; None:\n        \"\"\"Generate the validation modules for the specified .tsv or .csv file.\n\n        Args:\n            infile (str): the input .tsv or .csv file\n            is_tsv (bool): True if the file is a .tsv file, False if it is a .csv file\n        Returns:\n            None\n        \"\"\"\n        header_to_position_lookup = {}\n\n        header_to_position_lookup = self._derive_column_headers_for_tsv_file(infile, is_tsv=is_tsv)\n\n        self._generate_validator_class(header_to_position_lookup, infile)\n        self._generate_parser_class(header_to_position_lookup, infile)\n        self._generate_main_script(self.data_file_type, self.namespace, infile)\n        self._process_columns_for_tsv_file(infile, header_to_position_lookup, is_tsv=is_tsv)\n\n    def _generate_main_script(self, data_file_type: str, namespace: str, infile: str) -&gt; None:\n        \"\"\"Generate the main script that will be used to execute the validation.\n\n        Args:\n            data_file_type (str): the type of data file to be processed\n            namespace (str): the namespace where the modules will be located\n            infile (str): the source input file that was used to generate this validation component\n        \"\"\"\n        namespace_temp_dir = f\"{namespace.lower().replace('.', '-')}-validator\"\n        template_name = \"validate_file.py\"\n\n        lookup = {\n            \"namespace\": namespace,\n            \"namespace_temp_dir\": namespace_temp_dir,\n            \"data_file_type\": data_file_type\n        }\n\n        output = self._generate_output_from_template(template_name, lookup)\n\n        outfile = os.path.join(self.outdir, template_name)\n\n        self._write_class_file_from_template(template_name, outfile, output, infile)\n\n    def _generate_validator_class(\n        self, header_to_position_lookup: Dict[str, int], infile: str\n    ) -&gt; None:\n        \"\"\"Generate the validation module that will contain the Validator class that will drive the validation.\n\n        Args:\n            header_to_position_lookup (dict): key is the header name, value is the index position\n            infile (str): the source input file that was used to generate this validation component\n        \"\"\"\n\n        # Specify the name of the template file\n        template_name = \"validator.py\"\n\n        # Create a dictionary with data to be passed to the template\n        lookup = {}\n\n        for column_name, column_position in header_to_position_lookup.items():\n            attribute_name = self.column_name_to_attribute_name_lookup[column_name]\n            lookup[attribute_name] = column_position\n\n        data = {\"field_lookup\": lookup, \"file_type\": self.data_file_type}\n\n        output = self._generate_output_from_template(template_name, data)\n\n        namespace_dir = self.namespace.lower().replace(\".\", \"/\")\n        outdir = os.path.join(self.outdir, namespace_dir)\n        if not os.path.exists(outdir):\n            pathlib.Path(outdir).mkdir(parents=True, exist_ok=True)\n            logging.info(f\"Created output directory '{outdir}'\")\n\n        outfile = os.path.join(outdir, template_name)\n\n        self._write_class_file_from_template(template_name, outfile, output, infile)\n\n    def _generate_parser_class(\n        self, header_to_position_lookup: Dict[str, int], infile: str\n    ) -&gt; None:\n        \"\"\"Generate the parser module that will contain the Parser class that will provide a way to parse/read the file.\n\n        Args:\n            header_to_position_lookup (dict): key is the header name, value is the index position\n            infile (str): the source input file that was used to generate this parser.py file/module\n        \"\"\"\n\n        # Specify the name of the template file\n        template_name = \"parser.py\"\n\n        # Create a dictionary with data to be passed to the template\n        lookup = {}\n\n        for column_name, column_position in header_to_position_lookup.items():\n            attribute_name = self.column_name_to_attribute_name_lookup[column_name]\n            lookup[attribute_name] = column_position\n\n        data = {\"field_lookup\": lookup, \"file_type\": self.data_file_type}\n\n        output = self._generate_output_from_template(template_name, data)\n\n        namespace_dir = self.namespace.lower().replace(\".\", \"/\")\n        outdir = os.path.join(self.outdir, namespace_dir)\n        if not os.path.exists(outdir):\n            pathlib.Path(outdir).mkdir(parents=True, exist_ok=True)\n            logging.info(f\"Created output directory '{outdir}'\")\n\n        outfile = os.path.join(outdir, template_name)\n\n        self._write_class_file_from_template(template_name, outfile, output, infile)\n\n    def _process_columns_for_tsv_file(\n        self,\n        infile: str,\n        header_to_position_lookup: Dict[str, int],\n        is_tsv: bool = True\n    ) -&gt; None:\n        \"\"\"TBD.\"\"\"\n        lookup = {}\n        enum_lookup = {}\n\n        for column_name, column_position in header_to_position_lookup.items():\n            attribute_name = self.column_name_to_attribute_name_lookup[column_name]\n            logging.info(\n                f\"Processing column name '{column_name}' (with attribute name '{attribute_name}') at column position '{column_position}'\"\n            )\n\n            if attribute_name not in lookup:\n                class_name = self._derive_class_name_for_column_name(column_name)\n\n                lookup[attribute_name] = {\n                    \"datatype\": \"str\",\n                    \"column_name\": column_name,\n                    \"column_position\": column_position + 1,\n                    \"class_name\": class_name,\n                }\n\n            uniq_val_lookup = {}\n            uniq_val_ctr = 0\n            uniq_val_list = []\n\n            delimiter=\"\\t\"\n            if not is_tsv:\n                delimiter = \",\"\n\n            with open(infile) as f:\n                reader = csv.reader(f, delimiter=delimiter)\n                row_ctr = 0\n                for row in reader:\n                    row_ctr += 1\n                    if row_ctr == 1:\n                        continue\n                    else:\n                        if len(row) == 0:\n                            # Blank line to be skipped?\n                            continue\n                        # print(f\"{row=}\")\n                        val = row[column_position]\n                        if val not in uniq_val_lookup:\n                            uniq_val_lookup[val] = 0\n                            uniq_val_list.append(val)\n                            uniq_val_ctr += 1\n                        uniq_val_lookup[val] += 1\n\n            datatype = self._determine_datatype(uniq_val_list)\n\n            if datatype == \"different\":\n                lookup[attribute_name][\"datatype\"] = \"str\"\n            else:\n                lookup[attribute_name][\"datatype\"] = datatype\n\n            if uniq_val_ctr &lt;= self.max_equality_values:\n                logging.info(\n                    f\"Will generate enum class for attribute '{attribute_name}' for column '{column_name}' because the max unique values is '{uniq_val_ctr}'\"\n                )\n                class_name = lookup[attribute_name][\"class_name\"]\n                self._load_enum_lookup(uniq_val_lookup, enum_lookup, class_name)\n                lookup[attribute_name][\"uniq_values\"] = []\n                for uniq_val in uniq_val_lookup:\n                    lookup[attribute_name][\"uniq_values\"].append(uniq_val)\n\n            self._write_column_report_file(\n                column_name,\n                column_position,\n                infile,\n                uniq_val_ctr,\n                uniq_val_lookup,\n                row_ctr,\n            )\n\n        self._generate_record_class(lookup, enum_lookup, infile)\n\n    def _load_enum_lookup(self, uniq_val_lookup, enum_lookup, class_name) -&gt; None:\n        \"\"\"Load values into the enum lookup for this class.\n\n        Args:\n            TODO\n        \"\"\"\n        if class_name not in enum_lookup:\n            enum_lookup[class_name] = {}\n\n        for val in uniq_val_lookup:\n            enum_name = self._derive_attribute_name(val)\n\n            enum_name = enum_name.upper()\n\n            if len(enum_name) == 1 or re.search(r\"^\\d\", val):\n                enum_name = f\"{class_name.upper()}_{val.upper()}\"\n\n            logging.info(f\"{enum_name=} {val=}\")\n\n            enum_name = (\n                enum_name.replace(\" \", \"\")\n                .replace(\"*\", \"\")\n                .replace(\"\\\\\", \"\")\n                .replace(\"/\", \"_\")\n                .replace(\"|\", \"_\")\n                .replace(\"(\", \"_\")\n                .replace(\")\", \"_\")\n                .replace(\".\", \"_\")\n                .replace(\"-\", \"_\")\n            )\n\n            enum_lookup[class_name][enum_name] = val\n\n    def _write_column_report_file(\n        self,\n        column_name,\n        column_position,\n        infile,\n        uniq_val_ctr,\n        uniq_val_lookup,\n        row_ctr,\n    ) -&gt; None:\n        \"\"\"Write the report file for the column.\n\n        Args:\n            TODO\n        Returns:\n            None\n        \"\"\"\n        outfile = self._derive_column_outfile(column_name, column_position)\n\n        total_row_count = row_ctr - 1\n\n        with open(outfile, \"w\") as of:\n            of.write(f\"## method-created: {os.path.abspath(__file__)}\\n\")\n            of.write(\n                f\"## date-created: {str(datetime.today().strftime('%Y-%m-%d-%H%M%S'))}\\n\"\n            )\n            of.write(f\"## created-by: {os.environ.get('USER')}\\n\")\n            of.write(f\"## infile: {infile}\\n\")\n            of.write(f\"## logfile: {self.logfile}\\n\")\n\n            of.write(f\"Column name: '{column_name}'\\n\")\n            of.write(f\"Column position: '{column_position}'\\n\")\n            of.write(f\"Number of data rows: '{total_row_count}'\\n\")\n            of.write(f\"Here are the unique '{uniq_val_ctr}' values:\\n\")\n\n            for val, count in uniq_val_lookup.items():\n                percent = count / total_row_count * 100\n                of.write(f\"value: '{val}'; count: {count}; percentage: {percent}\\n\")\n\n        logging.info(f\"Wrote column report file '{outfile}'\")\n        if self.verbose:\n            print(f\"Wrote column report file '{outfile}'\")\n\n    def _derive_column_outfile(self, column_name: str, column_position: int) -&gt; str:\n        \"\"\"Derive the output file for the column-specific values.\n\n        Args:\n            column_name (str): the column name\n        Returns:\n            str: the output file\n        \"\"\"\n        basename = (\n            column_name.replace(\" \", \"\")\n            .replace(\"*\", \"\")\n            .replace(\"\\\\\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"|\", \"_\")\n            .replace(\"(\", \"_\")\n            .replace(\")\", \"_\")\n        )\n        outfile = os.path.join(self.outdir, f\"{column_position}_{basename}.tsv\")\n        return outfile\n\n    def _derive_column_headers_for_tsv_file(self, infile: str, is_tsv: bool = True) -&gt; Dict[str, int]:\n        \"\"\"Derive the column headers for the input .tsv file.\n\n        Args:\n            infile (str): the file to be parsed\n        Returns:\n            dict: column header is the key and column number is the value\n        \"\"\"\n        lookup = {}\n        column_ctr = 0\n        column_name_to_attribute_name_lookup = {}\n\n        delimiter=\"\\t\"\n        if not is_tsv:\n            delimiter = \",\"\n\n        with open(infile) as f:\n            reader = csv.reader(f, delimiter=delimiter)\n            row_ctr = 0\n            for row in reader:\n                row_ctr += 1\n                if row_ctr == 1:\n                    for field in row:\n                        lookup[field] = column_ctr\n                        attribute_name = self._derive_attribute_name(field)\n                        column_name_to_attribute_name_lookup[field] = attribute_name\n                        column_ctr += 1\n                    logging.info(f\"Processed the header of .tsv file '{infile}'\")\n                    break\n        logging.info(f\"Found '{column_ctr}' columns in file '{infile}'\")\n        self.column_name_to_attribute_name_lookup = column_name_to_attribute_name_lookup\n        return lookup\n\n    def _derive_attribute_name(self, column_name: str) -&gt; str:\n        \"\"\"Derive the attribute name for the column name.\n\n        This will remove special characters and spaces and lowercase the string.\n        Args:\n            column_name (str): the column name\n        Returns:\n            str: the attribute name\n        \"\"\"\n        # Use re.sub to replace all matches with an empty string\n        attribute_name = re.sub(self.pattern, \"\", column_name)\n        attribute_name = attribute_name.lower().replace(\" \", \"\")\n        return attribute_name\n\n    def _snake_to_upper_camel(self, class_name: str):\n        words = class_name.split(\"_\")\n        camel_case_words = [word.capitalize() for word in words]\n        return \"\".join(camel_case_words)\n\n    def _derive_class_name_for_column_name(self, column_name: str) -&gt; str:\n        \"\"\"Derive the class name for the column name.\n\n        This will remove special characters and spaces.\n        Args:\n            column_name (str): the column name\n        Returns:\n            str: the class name\n        \"\"\"\n        class_name = (\n            column_name.replace(\" \", \"_\")\n            .replace(\"*\", \"\")\n            .replace(\"#\", \"\")\n            .replace(\"\\\\\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"|\", \"_\")\n            .replace(\"(\", \"_\")\n            .replace(\")\", \"_\")\n        )\n\n        return self._snake_to_upper_camel(class_name)\n\n    def _generate_record_class(\n        self,\n        lookup: Dict[str, Dict[str, str]],\n        enum_lookup: Dict[str, Dict[str, str]],\n        infile: str,\n    ) -&gt; None:\n        \"\"\"TODO.\"\"\"\n        # Specify the name of the template file\n        template_name = \"record.py\"\n\n        # Create a dictionary with data to be passed to the template\n        data = {\n            \"lookup\": lookup,\n            \"file_type\": self.data_file_type,\n            \"enum_lookup\": enum_lookup,\n        }\n\n        output = self._generate_output_from_template(template_name, data)\n\n        namespace_dir = self.namespace.lower().replace(\".\", \"/\")\n        outdir = os.path.join(self.outdir, namespace_dir)\n        if not os.path.exists(outdir):\n            pathlib.Path(outdir).mkdir(parents=True, exist_ok=True)\n            logging.info(f\"Created output directory '{outdir}'\")\n\n        outfile = os.path.join(outdir, template_name)\n\n        self._write_class_file_from_template(template_name, outfile, output, infile)\n\n    def _generate_output_from_template(\n        self, template_name: str, data: Dict[str, Dict]\n    ) -&gt; str:\n        \"\"\"TODO.\"\"\"\n        # Load the template\n        template = self.env.get_template(template_name)\n        # Render the template with the data\n        output = template.render(data)\n\n        return output\n\n    def _write_class_file_from_template(\n        self, template_name: str, outfile: str, output: str, infile: str\n    ) -&gt; None:\n        with open(outfile, \"w\") as of:\n            of.write(f'\"\"\" method-created: {os.path.abspath(__file__)}\\n')\n            of.write(\n                f\"date-created: {str(datetime.today().strftime('%Y-%m-%d-%H%M%S'))}\\n\"\n            )\n            of.write(f\"created-by: {os.environ.get('USER')}\\n\")\n            of.write(f\"infile: {infile}\\n\")\n            of.write(f'logfile: {self.logfile}\\n\"\"\"\\n')\n\n            of.write(f\"{output}\\n\")\n\n        logging.info(f\"Wrote {template_name} file '{outfile}'\")\n        if self.verbose:\n            print(f\"Wrote {template_name} file '{outfile}'\")\n\n    def _determine_datatype(self, values: List[Any]) -&gt; str:\n        # Check if the array is not empty\n        if not values:\n            logging.error(\"values array is empty\")\n            sys.exit(1)\n\n        first_value = values[0]\n        first_datatype = None\n        first_datatype_clean = None\n\n        if self._is_convertible_to_int(first_value):\n            first_value = int(first_value)\n            first_datatype = \"int\"\n            first_datatype_clean = \"int\"\n        elif self._is_convertible_to_float(first_value):\n            first_value = float(first_value)\n            first_datatype = \"float\"\n            first_datatype_clean = \"float\"\n        else:\n            # Get the datatype of the first element\n            first_datatype = type(first_value)\n            first_datatype_clean = str(type(values[0])).split(\"'\")[1]\n\n        different = True\n\n        # Iterate through the array starting from the second element\n        for value in values[1:]:\n            current_datatype = None\n            # Check if the datatype of the current element matches the first datatype\n            if self._is_convertible_to_int(value):\n                value = int(value)\n                if first_datatype == \"int\":\n                    continue\n            elif self._is_convertible_to_float(value):\n                value = float(value)\n                if first_datatype == \"float\":\n                    continue\n\n            if type(value) != first_datatype:\n                logging.info(\n                    f\"values does not have a consistent datatype. Expected {first_datatype}, but found {type(value)}.\"\n                )\n                return \"different\"\n\n        # If the loop completes without returning, all elements have the same datatype\n        logging.info(\n            f\"All elements in the values array have the datatype: {first_datatype}\"\n        )\n        return first_datatype_clean\n\n    def _is_convertible_to_int(self, value):\n        try:\n            # Try converting the string to an integer\n            int_value = int(value)\n            logging.info(f\"{value} can be safely converted into an integer value\")\n            return True\n        except ValueError:\n            # Conversion failed\n            logging.info(f\"{value} cannot be safely converted into an integer value\")\n            return False\n\n    def _is_convertible_to_float(self, value):\n        try:\n            # Try converting the string to a float\n            float_value = float(value)\n            logging.info(f\"{value} can be safely converted into an float value\")\n            return True\n        except ValueError:\n            # Conversion failed\n            logging.info(f\"{value} cannot be safely converted into an float value\")\n            return False\n</code></pre>"},{"location":"manager/#validation_component_bootstrap_utils.manager.Manager.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Constructor for Manager.</p> Source code in <code>validation_component_bootstrap_utils/manager.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor for Manager.\"\"\"\n    self.config = kwargs.get(\"config\", None)\n    self.config_file = kwargs.get(\"config_file\", None)\n    self.data_file_type = kwargs.get(\"data_file_type\", None)\n    self.logfile = kwargs.get(\"logfile\", None)\n    self.namespace = kwargs.get(\"namespace\", None)\n    self.outdir = kwargs.get(\"outdir\", None)\n    self.template_path = kwargs.get(\"template_path\", None)\n    self.verbose = kwargs.get(\"verbose\", DEFAULT_VERBOSE)\n\n    # Define a regular expression pattern to match special characters\n    self.pattern = r\"[^a-zA-Z0-9\\s]\"  # This pattern will keep alphanumeric characters and whitespace\n\n    self.column_name_to_attribute_name_lookup = {}\n    self.max_equality_values = self.config[\"max_equality_values\"]\n\n    self._init_templating_system()\n\n    logging.info(f\"Instantiated Manager in file '{os.path.abspath(__file__)}'\")\n</code></pre>"},{"location":"manager/#validation_component_bootstrap_utils.manager.Manager.generate_validation_modules","title":"<code>generate_validation_modules(infile)</code>","text":"<p>Generate the validation modules for the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>infile</code> <code>str</code> <p>the input tab-delimited or csv file</p> required <p>Returns:     None</p> Source code in <code>validation_component_bootstrap_utils/manager.py</code> <pre><code>def generate_validation_modules(self, infile: str) -&gt; None:\n    \"\"\"Generate the validation modules for the specified file.\n\n    Args:\n        infile (str): the input tab-delimited or csv file\n    Returns:\n        None\n    \"\"\"\n    logging.info(\n        f\"Will attempt to generate validation modules for input file '{infile}'\"\n    )\n    if not os.path.exists(infile):\n        raise Exception(f\"file '{infile}' does not exist\")\n\n    extension = os.path.splitext(infile)[1]\n\n    if extension == \".csv\":\n        self._generate_validation_modules_for_tsv_file(infile, is_tsv=False)\n    elif extension == \".tsv\":\n        self._generate_validation_modules_for_tsv_file(infile, is_tsv=True)\n    else:\n        logging.error(\n            f\"Support does not exist for files with extension '{extension}'\"\n        )\n        sys.exit(1)\n</code></pre>"}]}